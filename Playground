import cProfile
from multiprocessing import cpu_count
from random import choice
import numpy as np
import matplotlib.pyplot as plt
from itertools import combinations
from matplotlib.backends.backend_pdf import PdfPages
import math
import cvxpy as cp
from scipy.stats import ortho_group

def std_matrix(mat):
    row_norms = np.linalg.norm(mat, axis=1, keepdims=True)
    row_norms[row_norms == 0] = 1.0
    return mat / row_norms

def bi_sign(vec):
    return np.where(vec >= 0, 1, -1)

def hoffman_constant(A):
    A = np.array(A)
    A_inv = np.linalg.inv(A)
    norm_A = np.linalg.norm(A, ord=2)
    norm_A_inv = np.linalg.norm(A_inv, ord=2)
    norm_A_fro = np.linalg.norm(A, 'fro')
    hoffman_const = (norm_A_inv * norm_A) / norm_A_fro
    return hoffman_const

def generate_ill_conditioned_matrix(m, n):
    U = np.array(ortho_group.rvs(dim=m))
    V = np.array(ortho_group.rvs(dim=n))
    S = np.zeros((m, n))
    random_numbers = [np.random.uniform(0.1, 10) for _ in range(min(m, n))]
    random_numbers = sorted(random_numbers)
    for i in range(len(random_numbers)):
        S[i, i] = random_numbers[i]
    S[min(m, n) - 1, min(m, n) - 1] = 0.000001
    return U @ S @ V.T

def linear_feasibility_linprog_mod(A, y):
    m, n = A.shape

    x = cp.Variable(n)
    t = cp.Variable()

    constraints = [
        y[i] * (A[i, :] @ x) >= t for i in range(m)
    ]
    constraints += [
        cp.norm(x, 2) <= 1,
        t <= 1
    ]

    objective = cp.Maximize(t)
    prob = cp.Problem(objective, constraints)
    prob.solve()

    x_opt = x.value
    return x_opt

class Pairwise_SKM:
    def __init__(self,
                A: np.ndarray,
                x_star: np.ndarray,
                beta: int,
                lambd: float,
                k: int,
                method_type: str = "hadamard_with_pairwise",
                chebyshev_center=None,
                normalize: bool = True,
                optimize: bool = False):
        self.A = A
        self.x_star = x_star.reshape(-1)
        self.beta = beta
        self.lambd = lambd
        self.k = k
        self.method_type = method_type
        self.chebyshev_center = chebyshev_center
        self.normalize = normalize
        self.optimize = optimize
        self.b_sign = self._bi_sign(A @ self.x_star)
        self.main_matrix = self._hadamard_product(A, self.b_sign)
        self.additional_matrix = self._prepare_additional_matrix()

    def _bi_sign(self, vec: np.ndarray) -> np.ndarray:
        return np.where(vec >= 0, 1, -1)

    def _std_matrix(self, mat: np.ndarray) -> np.ndarray:
        row_norms = np.linalg.norm(mat, axis=1, keepdims=True)
        row_norms[row_norms == 0] = 1.0
        return mat / row_norms

    def _hadamard_product(self, A: np.ndarray, b: np.ndarray) -> np.ndarray:
        m, n = A.shape
        b = b.ravel()

        B = []
        for i in range(n):
            B.append(b)

        B = np.array(B)
        B = np.transpose(B)
        return -1 * A * B

    def _construct_the_most_orthogonal(self, A: np.ndarray, x_star: np.ndarray) -> np.ndarray:
        m, n = A.shape
        pairwise_rows = []
        for i in range(m):
            for j in range(m):
                if i != j:
                    pairwise_rows.append(A[i, :] - A[j, :])

        row_abs_proj = []
        for row in pairwise_rows:
            row_abs_proj.append((abs(row @ x_star), row))

        row_abs_proj.sort(key=lambda x: x[0], reverse=True)
        selected_rows = [item[1] for item in row_abs_proj[:m]]
        return np.array(selected_rows)

    def sanity_check_matrix(self, A: np.ndarray, x_star: np.ndarray) -> np.ndarray:
        m, n = A.shape
        row_abs_proj = []
        for j in range(m):
            row_abs_proj.append((abs(A[j, :] @ x_star), A[j, :]))

        row_abs_proj.sort(key=lambda x: x[0], reverse=True)
        selected_rows = [item[1] for item in row_abs_proj[:max(m, 2 * n)]]
        return np.array(selected_rows)

    def _construct_the_most_orthogonal(self, A: np.ndarray, x_star: np.ndarray) -> np.ndarray:
        m, n = A.shape
        pairwise_rows = []
        for i in range(m):
            for j in range(m):
                if i != j:
                    pairwise_rows.append(A[i, :] - A[j, :])

        row_abs_proj = []
        for row in pairwise_rows:
            row_abs_proj.append((abs(row @ x_star), row))

        row_abs_proj.sort(key=lambda x: x[0], reverse=True)
        selected_rows = [item[1] for item in row_abs_proj[:m]]
        return np.array(selected_rows)

    def _prepare_additional_matrix(self) -> np.ndarray or None: # type: ignore
        A = self.A
        x_star = self.x_star
        method = self.method_type

        if method == "hadamard_product":
            return None

        elif method in ("hadamard_with_pairwise", "hadamard_with_pairwise_optimized"):
            m, n = A.shape
            pairwise_rows = std_matrix(self._construct_the_most_orthogonal(A, x_star))
            return self._hadamard_product(pairwise_rows, self._bi_sign(pairwise_rows @ x_star))

        elif method == "hadamard_gaussian_promax":
            m, n = A.shape
            pairwise_rows = []
            for i in range(m * 10):
                set_of_indexes = np.random.choice(m, 2, replace=False)
                pairwise_rows.append(A[set_of_indexes[0], :] - A[set_of_indexes[1], :])

            gaussian_rows = self._std_matrix(pairwise_rows)
            return self._hadamard_product(gaussian_rows, self._bi_sign(gaussian_rows @ x_star))

        elif method == "reduced_matrix":
            m, n = A.shape
            fixed_size = max(100, int(m * 0.05))
            set_of_indexes = np.random.choice(m, fixed_size, replace=False)
            gaussian_rows = []
            for i in set_of_indexes:
                for j in set_of_indexes:
                    if i != j:
                        gaussian_rows.append(A[i, :] - A[j, :])
            gaussian_rows = self._std_matrix(np.array(gaussian_rows))
            return self._hadamard_product(gaussian_rows, self._bi_sign(gaussian_rows @ x_star))

        else:
            return None

    def _changing_lambda(self, x, lambd):
        return 2 - 1 / (1 + math.exp(-lambd + x))

    def run(self):
        if self.normalize:
            norm_x_star = np.linalg.norm(self.x_star)
            if norm_x_star != 0:
                self.x_star = self.x_star / norm_x_star

        m, n = self.A.shape
        main_matrix = self.main_matrix
        add_matrix = self.additional_matrix

        xk = np.ones(n)
        xk_history = [xk.copy()]
        accuracy_history = []
        error_history = []

        initial_signs = self._bi_sign(main_matrix @ self.x_star).ravel()

        for it in range(1, self.k + 1):
            idx = np.random.choice(m, self.beta, replace=False)
            product_main = main_matrix[idx, :] @ xk

            max_idx_main = np.argmax(product_main)
            min_idx_main = np.argmin(product_main)

            max_row = main_matrix[idx[max_idx_main]]
            max_val = product_main[max_idx_main]

            min_row = main_matrix[idx[min_idx_main]]
            min_val = product_main[min_idx_main]

            if add_matrix is not None:
                add_idx = np.random.choice(add_matrix.shape[0], self.beta, replace=False)
                product_add = add_matrix[add_idx, :] @ xk

                max_idx_add = np.argmax(product_add)
                min_idx_add = np.argmin(product_add)

                if product_add[max_idx_add] > max_val:
                    max_val = product_add[max_idx_add]
                    max_row = add_matrix[add_idx[max_idx_add]]

                if product_add[min_idx_add] < min_val:
                    min_val = product_add[min_idx_add]
                    min_row = add_matrix[add_idx[min_idx_add]]

            if self.optimize and (it % 10 == 0 or it % 11 == 0):
                if max_val > 0:
                    step_size = max_val ** 2 * self.lambd
                    xk = xk - step_size * min_row
                else:
                    pass
            else:
                if max_val > 0:
                    xk = xk - (self.lambd * max_val) * max_row

            if self.normalize:
                norm_xk = np.linalg.norm(xk)
                if norm_xk != 0:
                    xk = xk / norm_xk

            xk_history.append(xk.copy())
            current_signs = self._bi_sign(main_matrix @ xk).ravel()
            accuracy = np.sum(current_signs == initial_signs) / m
            accuracy_history.append(accuracy)

            error_val = np.linalg.norm(xk - self.x_star) ** 2
            error_history.append(error_val)

        return xk_history, accuracy_history, error_history

def run_experiments():
    m = 1000
    n = 10
    k = 2000

    G = std_matrix(generate_ill_conditioned_matrix(m, n))
    x_star = np.random.uniform(-10, 10, (n, 1))

    chebyshev_center = linear_feasibility_linprog_mod(G, G @ x_star)

    types = [
        "hadamard_product",
        "hadamard_with_pairwise",
        "hadamard_gaussian_promax",
        "reduced_matrix",
        "hadamard_with_pairwise_optimized",
    ]

    labels = {
        "hadamard_product": "-b * Ax <= 0",
        "hadamard_with_pairwise": "-b * Ax <= 0 ∪ Pairwise",
        "hadamard_gaussian_promax": "-b * Ax <= 0 ∪ G'",
        "reduced_matrix": "-b * A_reduced * x <= 0 ∪ Pairwise",
        "hadamard_with_pairwise_optimized": "-b * Ax <= 0 ∪ Pairwise + optimized"
    }

    lambda_values = [2]
    beta_values = [10]

    with PdfPages('output_plots.pdf') as pdf:
        for lmbd in lambda_values:
            for beta in beta_values:

                histories = {
                    "accuracy": {},
                    "error": {}
                }

                print(f"\nRunning experiments for lambda (alpha) = {lmbd} and beta = {beta}")

                for method_type in types:
                    print(f"  Running SKM for type: {labels[method_type]}")
                    accuracy_experiments = []
                    error_experiments = []
                    optimize_flag = (method_type == "hadamard_with_pairwise_optimized")

                    for _ in range(15):
                        skm_solver = Pairwise_SKM(
                            A=G,
                            x_star=x_star,
                            beta=beta,
                            lambd=lmbd,
                            k=k,
                            method_type=method_type,
                            chebyshev_center=chebyshev_center,
                            normalize=True,
                            optimize=optimize_flag
                        )

                        xk_history, accuracy_history, error_history = skm_solver.run()
                        accuracy_experiments.append(accuracy_history)
                        error_experiments.append(error_history)

                    avg_accuracy = np.mean(accuracy_experiments, axis=0)
                    avg_error = np.mean(error_experiments, axis=0)

                    histories["accuracy"][labels[method_type]] = avg_accuracy
                    histories["error"][labels[method_type]] = avg_error

                plt.figure(figsize=(14, 6))

                plt.subplot(1, 2, 1)
                for label in labels.values():
                    plt.plot(range(1, k + 1), histories["accuracy"][label], label=label)
                plt.xlabel('Iterations')
                plt.ylabel('Accuracy')
                plt.title(f'Accuracy (lambda={lmbd}, beta={beta})')
                plt.legend()
                plt.grid(True)

                plt.subplot(1, 2, 2)
                for label in labels.values():
                    plt.plot(range(1, k + 1), histories["error"][label], label=label)
                plt.xlabel('Iterations')
                plt.ylabel('Approximation Error ||xk - x*||²')
                plt.yscale('log')
                plt.ylim(1e-12, 1e2)
                plt.title(f'Error (lambda={lmbd}, beta={beta})')
                plt.legend()
                plt.grid(True, which="both", ls="--", linewidth=0.5)

                plt.tight_layout()
                pdf.savefig()
                plt.close()

                print(f"\nFinal Results after {k} iterations (lambda={lmbd}, beta={beta}):")
                for label in labels.values():
                    final_accuracy = histories["accuracy"][label][-1]
                    final_error = histories["error"][label][-1]
                    print(f"  Type {label}:")
                    print(f"    Accuracy: {final_accuracy * 100:.2f}%")
                    print(f"    Approximation Error: {final_error:.12f}\n")

    print("\nAll plots have been saved to 'output_plots.pdf'.")

if __name__ == "__main__":
    run_experiments()
